---
title: "Graphs and Language"
lastmod: "2024-02-15"
summary: >
  In this blog post, we will explore how Knowledge Graphs can benefit from LLMs, and vice versa.
categories:
  - ML
series:
  -
image: /images/graphs-and-language/talk.png
---

**A rising tide lifts all boats**, and the recent advances in LLMs are no exception.
In this blog post, we will explore how Knowledge Graphs can benefit from LLMs, and vice versa.

<figure>
  <img
    src="/images/graphs-and-language/KGs_and_LLMs.png"
    alt="Where do KGs fit with LLMs?"
  />
  <figcaption>
    Where do Knowledge Graphs fit with Large Language Models?
  </figcaption>
</figure>

In particular, Knowledge Graphs can ground LLMs with facts using Graph RAG, which can be cheaper than Vector RAG.
We'll look at a 10-line code example in LlamaIndex and see how easy it is to start.
LLMs can help build automated KGs, which have been a bottleneck in the past.
Graphs can provide your Domain Experts with an interface to supervise your AI systems.

_Note: this is a written version of a talk I gave at the [AI in Production online conference](https://home.mlops.community/home/events/ai-in-production-2024-02-15) on February 15th, 2024.
You can watch the talk here (link coming)._

<figure>
  <img src="/images/graphs-and-language/recap.png" alt="Talk overview" />
  <figcaption>Overview of the talk</figcaption>
</figure>

## A trip down memory lane at Spacy IRL 2019

I've been working with Natural Language Processing for a few years now, and I've seen the rise of Large Language Models.
The start of my NLP and Graphs work dates back to 2018, applied to the Sports Media domain
when I worked as a Machine Learning Engineer at OneFootball, a football media company from Berlin, Germany.

As a practitioner, I remember that time well because it was a time of great change in the NLP field.
We were moving from the era of rule-based systems and word embeddings to the era of deep learning,
moving from LSTMs to a slew of models like Elmo or ULMfit based on the transformer architecture.
I was one of the lucky few who could attend the [Spacy IRL 2019](https://irl.spacy.io/2019/) conference in Berlin.
There were corporate training workshops followed by talks about Transformers, conversational AI assistants, and applied NLP in finance or media.

<div
  style={{
    display: "flex",
    justifyContent: "space-between",
  }}
>
  <figure style={{ width: "65%" }}>
    <img src="/images/graphs-and-language/spacy_irl.jpg" alt="Spacy IRL 2019" />
    <figcaption>Spacy IRL 2019 keynote by Sebastian Ruder</figcaption>
  </figure>
  <figure style={{ width: "30%" }}>
    <img
      src="/images/graphs-and-language/spacy_irl_louis.jpg"
      alt="Me at Spacy IRL"
    />
    <figcaption>
      Me standing in the background, me at Spacy IRL, networking with a ghost
    </figcaption>
  </figure>
</div>

In his keynote, [The missing elements in NLP (spaCy IRL 2019)](https://www.youtube.com/watch?v=e12danHhlic),
Yoav Goldberg predicts that the next big development will be to **enable non-experts to use NLP**.
He was right ✅.
He thought we would get there by **humans writing rules aided by Deep Learning resulting in transparent and debuggable models**.
He was wrong ❌.
We got there with chat, and we now have less transparent and less debuggable models.
We moved further right and down on his chart (see below) to a place deeper than Deep Learning.
The jury is still out on whether we can move towards more transparent models that work for non-experts and with little data.

<figure>
  <img
    src="/images/graphs-and-language/spacy_irl_keynote.png"
    alt="Spacy IRL Keynote"
  />
  <figcaption>
    Yoav Goldberg: The missing elements in NLP (spaCy IRL 2019)
  </figcaption>
</figure>

In the context of my employer at the time, OneFootball, a football media in 12 languages with 10 million monthly active users,
we used NLP to assist our newsroom and unlock new product features.
I built systems to extract entities and relations from football articles, tag the news, and recommend articles to users.
I shared some of that work in a [previous talk](https://www.youtube.com/watch?v=QvNv402WBSY&t=1057s) at a Berlin NLP meetup.
We had medium data, not a lot. And we had partial labels in the form of "retags".
We also could not pay for much compute. So we had to be creative.
It was the realm of **Applied NLP**.

That's where I stumbled upon the beautiful world of Graphs, specifically the great work from my now friend [Paco Nathan](https://derwen.ai/paco)
with his library <em>[pytextrank](https://spacy.io/universe/project/spacy-pytextrank)</em>.
Graphs (along with rule-based matchers, weak supervision, and other NLP tricks I applied over the years) helped me
work with **little** annotated data and incorporate **declarative knowledge** from domain experts
while building a system that could be used and maintained by non-experts, with some level of **human+machine collaboration**.
We shipped a much better tagging system and a new recommendation system, and I was hooked.

Today with the rise of LLMs, I see a lot of potential to combine the two worlds of Graphs and LLMs, and I want to share that with you.

## 1. Fact grounding with Graph RAG

### 1.1 Fine-tuning vs Retrieval-Augmented Generation

The first place where Graphs and LLMs meet is in the area of fact grounding.
LLMs suffer from a few issues like hallucination, knowledge cut-off, bias, and lack of control.
To circumvent those issues, people have turned to their available domain data.
In particular, two approaches emerged: Fine Tuning and Retrieval-Augmented Generation (RAG).

In his talk [LLMs in Production](https://www.youtube.com/watch?v=xa7k9MUeIdk) at the AI Conference 3 months ago,
Dr. Waleed Kadous, Chief Scientist at AnyScale, sheds some light on navigating the trade-offs between the two approaches.
**"Fine-tuning is for form, not facts"**, [he says](https://www.anyscale.com/blog/fine-tuning-is-for-form-not-facts). **"RAG is for facts"**.

Fine-tuning will get easier and cheaper. Open-source libraries like [OpenAccess-AI-Collective/axolotl](https://github.com/OpenAccess-AI-Collective/axolotl)
and [huggingface/trl](https://github.com/huggingface/trl) already make this process easier.
But, it's still resource-intensive and requires more NLP maturity as a business. RAG is more accessible, on the other hand.

According to this Hacker News thread from 2 months ago, [Ask HN: How do I train a custom LLM/ChatGPT on my documents in Dec 2023?](https://news.ycombinator.com/item?id=38759877),
the vast majority of practitioners are indeed using RAG rather than fine-tuning.

### 1.2 Vector RAG vs Graph RAG

When people say RAG, they usually mean Vector RAG, which is a retrieval system based on a Vector Database.
In their [blog post](https://www.nebula-graph.io/posts/graph-RAG) and accompanying [notebook tutorial](https://www.siwei.io/en/demos/graph-rag/),
NebulaGraph introduces an alternative that they call Graph RAG, which is a retrieval system based on a Graph Database
(disclaimer: they are a Graph database vendor).
They show that the facts retrieved by the RAG system will vary based on the chosen architecture.

They also show in a separate [tutorial part of the LlamaIndex docs](https://docs.llamaindex.ai/en/v0.9.10/examples/index_structs/knowledge_graph/KnowledgeGraphIndex_vs_VectorStoreIndex_vs_CustomIndex_combined.html#comparison-of-results) that
Graph RAG is **more concise** and hence **cheaper** in terms of tokens than Vector RAG.

### 1.3 RAG Zoo

To make sense of the different RAG architectures, consider the following diagrams I created:

<figure>
  <img src="/images/graphs-and-language/rag.png" alt="RAG Zoo" />
  <figcaption>Differences and similarities of the RAG architectures</figcaption>
</figure>

In all cases, we ask a question in natural language Q<sub>NL</sub>
and we get an answer in natural language A<sub>NL</sub>.
In all cases, a LLM is used to generate the answer ("Answer Gen").

**Vector RAG** embeds the query (usually with a smaller model than the LLM; something like FlagEmbeddings or any small of the models
at the top of the [Huggingface Embeddings Leaderboard](https://huggingface.co/spaces/mteb/leaderboard)) into a vector v<sub>Q</sub>.
It then retrieves the top-k document chunks from the Vector DB that are closest to v<sub>Q</sub> and returns the vectors and chunks (v<sub>j</sub>, C<sub>j</sub>).
Those are passed along with Q<sub>NL</sub> to the LLM, which generates the answer A<sub>NL</sub>.

**Graph RAG** extracts the keywords from the query k<sub>i</sub> and retrieves triples from the graph that match the keyword.
It then passes the triples (s<sub>j</sub>, p<sub>j</sub>, o<sub>j</sub>) along with Q<sub>NL</sub> to the LLM, which generates the answer A<sub>NL</sub>.

**Structured RAG** uses a model (LLM or smaller fine-tuned model) to generate a query in the database's query language.
It could generate a SQL query for a RDBMS or a Cypher query for a Graph DB.
For example, let's imagine we query a RDBMS. The model will generate Q<sub>SQL</sub> which is then passed to the database to retrieve the answer.
We note the answer A<sub>SQL</sub> but those are data records that result from running Q<sub>SQL</sub> in the database.
The answer A<sub>SQL</sub> as well as Q<sub>NL</sub> are passed to the LLM to generate A<sub>NL</sub>.

In the case of **Hybrid RAG**, the system uses a combination of the above. There are multiple hybridation techniques that go beyond this blog post.
The simple idea is that you pass more context to the LLM for Answer Gen, and you let it use its summarisation strength to generate the answer.

### 1.4 Graph RAG implementation in LlamaIndex

And now for the code, with the current frameworks, we can build a Graph RAG system in 10 lines of python.

```python
from llama_index.llms import Ollama
from llama_index import ServiceContext, KnowledgeGraphIndex
from llama_index.retrievers import KGTableRetriever
from llama_index.graph_stores import Neo4jGraphStore
from llama_index.storage.storage_context import StorageContext
from llama_index.query_engine import RetrieverQueryEngine
from llama_index.data_structs.data_structs import KG
from IPython.display import Markdown, display

llm = Ollama(model='mistral', base_url="http://localhost:11434")
service_context = ServiceContext.from_defaults(llm=llm, embed_model="local:BAAI/bge-small-en")

graph_store = Neo4jGraphStore(username="neo4j", password="password", url="bolt://localhost:7687", database="neo4j")
storage_context = StorageContext.from_defaults(graph_store=graph_store)

kg_index = KnowledgeGraphIndex(index_struct=KG(index_id="vector"), service_context=service_context, storage_context=storage_context)
graph_rag_retriever = KGTableRetriever(index=kg_index, retriever_mode="keyword")

kg_rag_query_engine = RetrieverQueryEngine.from_args(retriever=graph_rag_retriever, service_context=service_context)

response_graph_rag = kg_rag_query_engine.query("Tell me about Peter Quill.")
display(Markdown(f"<b>{response_graph_rag}</b>"))
```

This snippet supposes you have Ollama serving the [mistral](https://huggingface.co/mistralai/Mistral-7B-v0.1) model and a Neo4j database running locally.
It also assumes you have a Knowledge Graph in your Neo4j database, but if you don't we'll cover in the next section how to build one.

## 2. KG construction

### 2.1 Building a Knowledge Graph

Before conducting inference, you need to index your data either in a Vector DB or a Graph DB.

<figure>
  <img src="/images/graphs-and-language/indexing.png" alt="Indexing DBs" />
  <figcaption>Indexing architectures for RAG</figcaption>
</figure>

The equivalent of chunking and embedding documents for Vector RAG is extracting Triples for Graph RAG.
Triples are of the form (s, p, o) where s is the subject, p is the predicate, and o is the object.
Subjects and objects are entities, and predicates are relationships.

There are a few ways to extract Triples from text, but the most common way is to use a **Named Entity Recogniser (NER)** and a **Relation Extractor (RE)**.
NER will extract entities like "Peter Quill" and "Guardians of the Galaxy vol 3", and RE will extract relationships like "plays role in" and "directed by".

There are fine-tuned models specialised in RE like [REBEL](https://huggingface.co/Babelscape/mrebel-large), but people started using LLMs to extract triples.
Here is the default prompt chain of LlamaIndex for RE:

```jinja
Some text is provided below. Given the text, extract up to
{max_knowledge_triplets}
knowledge triplets in the form of (subject, predicate, object). Avoid stopwords.
---------------------
Example:
Text: Alice is Bob's mother.
Triplets: (Alice, is mother of, Bob)
Text: Philz is a coffee shop founded in Berkeley in 1982.
Triplets:
(Philz, is, coffee shop)
(Philz, founded in, Berkeley)
(Philz, founded in, 1982)
---------------------
Text: {text}
Triplets:
```

The issue with this approach is that first you have to parse the chat output with regexes,
and second you have no control over the quality of entities or relationships extracted.

### 2.2 KG construction implementation in LlamaIndex

With LlamaIndex however, you can build a KG in 10 lines of python using the following code snippet:

```python
from llama_index.llms import Ollama
from llama_index import ServiceContext, KnowledgeGraphIndex
from llama_index.graph_stores import Neo4jGraphStore
from llama_index.storage.storage_context import StorageContext
from llama_index import download_loader

llm = Ollama(model='mistral', base_url="http://localhost:11434")
service_context = ServiceContext.from_defaults(llm=llm, embed_model="local:BAAI/bge-small-en")

graph_store = Neo4jGraphStore(username="neo4j", password="password", url="bolt://localhost:7687", database="neo4j")
storage_context = StorageContext.from_defaults(graph_store=graph_store)

loader = download_loader("WikipediaReader")()
documents = loader.load_data(pages=['Guardians of the Galaxy Vol. 3'], auto_suggest=False)

kg_index = KnowledgeGraphIndex.from_documents(
    documents,
    storage_context=storage_context,
    service_context=service_context,
    max_triplets_per_chunk=5,
    include_embeddings=False,
    kg_triplet_extract_fn=None,
    kg_triple_extract_template=None
)
```

### 2.3 Example failure modes of LLM-based KG construction

However, if we have a look at the resulting KG for the movie "Guardians of the Galaxy vol 3", we can note a few issues.

<figure>
  <img
    src="/images/graphs-and-language/kg_construction.png"
    alt="Example constructed KG with LLM"
  />
  <figcaption>Neo4j Bloom screenshot of a KG constructed with a LLM</figcaption>
</figure>

Here is a table overview of the issues

| #   | Observed                                                                                                                                           | Expected                                                                                                                                                                                   | Comment                                                                                    |
| --- | -------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------ |
| 1.  | "Peter Quill / star-lord" vs "Quill" or "Guardians of the Galaxy" vs "Vol. 3" are separate entities                                                | Different synonyms should still disambiguate to the same entity                                                                                                                            | **Entity Linking** systems are used to disambiguate entities via collected "surface forms" |
| 2.  | "plays role in" and "is part of the cast in" are different relationships that mean the same thing                                                  | Relationships should be consistent or even better matching a provided controlled vocabulary                                                                                                | **Relation Extraction** systems are used to extract standardised relationships             |
| 3.  | triples (Quill, speaks uncensored language in, Guardians of the Galaxy) and (James Gunn, could not imagine, Guardians of the Galaxy) are imprecise | If a triple is found, it should resolve to the most important information. In this case (Quill, is present in, Guardians of the Galaxy) or (James Gunn, directed, Guardians of the Galaxy) | Could be mitigated by using a **controlled vocabulary** for relationships                  |

This is to be compared with the Wikidata graph labelled by humans, which looks like this:

<figure>
  <img src="/images/graphs-and-language/wikidata.png" alt="Human-labelled KG" />
  <figcaption>
    Human-labelled KG in Wikidata generated with
    [metaphacts](https://wikidata.metaphacts.com/resource/wd:Q29226331?view=graph)
  </figcaption>
</figure>

### 2.4 Towards better KG construction

So where do we go from there?
KGs are difficult to construct and evolve by nature, which challenges the existing methods
in KGs to generate new facts and represent unseen knowledge.
The paper [Unifying Large Language Models and Knowledge Graphs: A Roadmap](https://arxiv.org/pdf/2306.08302.pdf)
provides a good overview of the current state of the art and the challenges ahead.

> Knowledge graph construction involves creating a structured representation of knowledge within a specific domain.
> This includes identifying entities and their relationships
> with each other. The process of knowledge graph construction typically involves multiple stages, including 1) entity
> discovery, 2) coreference resolution, and 3) relation extraction.
> Fig 19 presents the general framework of applying LLMs for
> each stage in KG construction. More recent approaches have
> explored 4) end-to-end knowledge graph construction, which
> involves constructing a complete knowledge graph in one
> step or directly 5) distilling knowledge graphs from LLMs.

Which is summarised in this figure from the paper:

<figure>
  <img
    src="/images/graphs-and-language/llm_kg_construction.png"
    alt="The general framework of LLM-based KG construction"
  />
  <figcaption>The general framework of LLM-based KG construction</figcaption>
</figure>

I've seen only a few projects that have tried to tackle this problem: [DerwenAI/textgraphs](https://github.com/DerwenAI/textgraphs) and [IBM/zshot](https://github.com/IBM/zshot).

## 3. Unlock Experts

### 3.1 Human vs AI

The final place where Graphs and LLMs meet is **Human+Machine collaboration**.
Who doesn't love a "Human vs AI" story? News headlines about "AGI" or "ChatGPT passing the bar exam" are everywhere.

<figure style={{ width: "50%" }}>
  <img src="/images/graphs-and-language/human_vs_ai.png" alt="Human vs AI" />
  <figcaption>Human vs AI news headlines</figcaption>
</figure>

I would encourage the reader to have a look at [this answer](https://aisnakeoil.com/p/gpt-4-and-professional-benchmarks) from the AI Snake Oil newsletter.
They make a good point that models like ChatGPT **memorise the solutions rather than reason** about them, which makes exams a bad way to compare humans with machines.

Going beyond Memorisation, there is a whole area of research around what's called
Generalization, **Reasoning, Planning, Representation Learning**, and graphs can help with that.

### 3.2 Human + Machine: Visualisation

Rather than against each other, I'm interested in ways Humans and Machines can work together.
In particular, how do humans **understand and debug black-box models**?

One key project that, in my opinion, moved the needle there was the [whatlies paper](https://arxiv.org/abs/2009.02113) from Vincent Warmerdam, 2020.
He used UMAP on embeddings to reveal quality issues in LLMs, and built a framework for others to audit their embeddings rather than blindly trust them.

Similarly, Graph Databases come with a lot of visualisation tools out of the box. For example, they would add context with
colour, metadata, and different layout algorithms (force-based, Sankey).

<div
  style={{
    display: "flex",
    justifyContent: "space-between",
  }}
>
  <figure style={{ width: "45%" }}>
    <img src="/images/graphs-and-language/whatlies.jpg" alt="Whatlies" />
    <figcaption>Warmerdam, 2020: whatlies in embeddings</figcaption>
  </figure>
  <figure style={{ width: "45%" }}>
    <img src="/images/graphs-and-language/neo4j.png" alt="Neo4j Bloom" />
    <figcaption>Neo4j Bloom</figcaption>
  </figure>
</div>

### 3.3 Human + Machine: Human in the Loop

Finally, how do we **address the lack of control** of Deep Learning models, and how do we **incorporate declarative knowledge** from domain experts?

I like to refer to the phrase "the proof is in the pudding", and by that, I mean that the value of a piece of tech must be judged based on its results in production.
And when we look at production systems, we see that LLMs or Deep Learning models are not used in isolation, but rather within **Human-in-the-Loop systems**.

In a [project](https://security.googleblog.com/2024/01/scaling-security-with-ai-from-detection.html) and [paper](https://storage.googleapis.com/gweb-research2023-media/pubtools/pdf/4fd3441fe40bb74e3f94f5203a17399af07b115c.pdf) from 2 weeks ago,
Google has started using language models to help it find and spot bugs in its C/C++, Java, and Go code.
The results have been encouraging: it has recently started using an LLM based on its Gemini model to
“successfully fix 15% of sanitiser bugs discovered during unit tests, resulting in hundreds of bugs patched”.
Though the 15% acceptance rate sounds relatively small, it has a big effect at Google-scale.
The bug pipeline yields better-than-human fixes - “approximately 95% of the commits sent to code owners
were accepted without discussion,” Google writes. “This was a higher acceptance rate than human-generated
code changes, which often provoke questions and comments”.

The key takeaway here for me has to do with their architecture:

<figure>
  <img
    src="/images/graphs-and-language/google_project.png"
    alt="AI-powered patching"
  />
  <figcaption>AI-powered patching at Google</figcaption>
</figure>

They built it with a LLM, but they also combined LLMs with smaller more specific AI models,
and more importantly with a double human filter on top, thus working with machines.

## Conclusion

I remember those 2019 days vividly, moving from LSTMs to Transformers, and we thought _that_ was Deep Learning.
Now, with LLMs, we've reached what I would describe as **Abysmal Learning**.
And I like this image because it can mean both **"extremely deep"** as well as **"profoundly bad"**.

More than ever, we need more control, more transparency, and ways for humans to work with machines.
In this blog post, we've seen here a few ways in which Graphs and LLMs can work together to help with that, and
I'm excited to see what the future holds.

<figure>
  <img src="/images/graphs-and-language/abysmal.png" alt="Abysmal Learning" />
  <figcaption>Deeper than Deep Learning: Abysmal Learning</figcaption>
</figure>

## Resources

1. [Language, Graphs, and AI in industry](https://derwen.ai/s/mqqm) - Paco Nathan - Jan, 2024
1. [Graph ML meets Language Models](https://blog.derwen.ai/visual-missives-from-the-latent-space-2023-10-16-d4bfa944b86c) - Paco Nathan - Oct 25, 2023
1. [[2306.08302] Unifying Large Language Models and Knowledge Graphs: A Roadmap](https://arxiv.org/abs/2306.08302)
1. [GitHub - RManLuo/Awesome-LLM-KG: Awesome papers about unifying LLMs and KGs](https://github.com/RManLuo/Awesome-LLM-KG) - Jun 14, 2023
1. [Evaluating LLMs is a minefield](https://www.cs.princeton.edu/~arvindn/talks/evaluating_llms_minefield/)
1. [GPT-4 and professional benchmarks: the wrong answer to the wrong question](https://www.aisnakeoil.com/p/gpt-4-and-professional-benchmarks) - AI Snake Oil - Oct 4, 2023
1. [AI-powered patching: the future of automated vulnerability fixes](https://storage.googleapis.com/gweb-research2023-media/pubtools/pdf/4fd3441fe40bb74e3f94f5203a17399af07b115c.pdf) - Google Security - Jan 31, 2024
1. [Graph & Geometric ML in 2024: Where We Are and What’s Next (Part II — Applications)](https://towardsdatascience.com/graph-geometric-ml-in-2024-where-we-are-and-whats-next-part-ii-applications-1ed786f7bf63) | by Michael Galkin - Jan 16, 2024
1. [[2312.02783] Large Language Models on Graphs: A Comprehensive Survey](https://arxiv.org/abs/2312.02783) - Dec 5, 2023
1. [ULTRA: Foundation Models for Knowledge Graph Reasoning | by Michael Galkin | Towards Data Science](https://towardsdatascience.com/ultra-foundation-models-for-knowledge-graph-reasoning-9f8f4a0d7f09) - Nov 3, 2023
1. [Fine Tuning Is For Form, Not Facts | Anyscale](https://www.anyscale.com/blog/fine-tuning-is-for-form-not-facts) - July 5, 2023
1. [GenAI Stack Walkthrough: Behind the Scenes With Neo4j, LangChain, and Ollama in Docker](https://neo4j.com/developer-blog/genai-app-how-to-build/) - Oct 05, 2023
1. [NebulaGraph Launches Industry-First Graph RAG: Retrieval-Augmented Generation with LLM Based on Knowledge Graphs](https://www.nebula-graph.io/posts/graph-RAG) - Sep 6, 2023
1. [RAG Using Unstructured Data & Role of Knowledge Graphs | Kùzu](https://kuzudb.com/docusaurus/blog/llms-graphs-part-2/) - Jan 15, 2024
1. [Constructing knowledge graphs from text using OpenAI functions](https://bratanic-tomaz.medium.com/constructing-knowledge-graphs-from-text-using-openai-functions-096a6d010c17) | by Tomaz Bratanic - Oct 20, 2023
1. [Knowledge graph from unstructured text | by Noah Mayerhofer | Neo4j Developer Blog](https://medium.com/neo4j/construct-knowledge-graphs-from-unstructured-text-877be33300a2) - Sep 21, 2023
